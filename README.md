# Neural Networks: A Journey into Heat Flux & Perceptrons 🔥🏡

Welcome to my **Neural Network Playground**! If you're passionate about neural networks, data-driven decision making, or just curious about how houses stay warm, you've come to the right place.

This project is split into two exciting parts:

1. **Part 1**: Manual training of a multi-layer feedforward network using a hands-on, example-by-example approach. Yes, we *manually* train the network (because who doesn't love a good challenge?).
2. **Part 2**: Applying those neural network skills to predict **Heat Flux** into homes, helping architects design the warmest, coziest homes possible 🏠✨.

## 🧠 Project Highlights

- **Manual Feedforward Network Training**:
  - Walk through each step of network training, from weight updates to error calculations. It’s like a neural network workout session — **but with Python**.

- **Predicting Heat Flux**:
  - Train and fine-tune a **multi-layer perceptron (MLP)** to predict how much heat a house gets based on its insulation and orientation.
  - We use powerful optimization techniques like **Stochastic Gradient Descent** and the adaptive **Adagrad** optimizer.

- **Visualizations**:
  - Discover the relationships between insulation, orientation (East, South, North), and heat flux through snazzy plots! Some surprises await, like the strong **negative correlation** between the North orientation and heat influx. Who knew North could be so cold? 🥶

## 🎉 Why This Project is Special

Not only does this project show off technical chops in **neural networks**, but it also uncovers key insights about **building design** using real-world data. Plus, it’s loaded with code that’s fully annotated and easy to follow (because no one likes spaghetti code 🍝).

## ⚙️ Tools & Techniques

- **Languages & Libraries**: 
  - Python: `NumPy`, `Pandas`, `Matplotlib`, `Seaborn`, `TensorFlow`
  - Machine Learning: Feedforward Neural Networks, Multi-Layer Perceptrons
  - Optimization: **SGD (with learning rates & momentum)**, **Adagrad**
- **Evaluation Metrics**: 
  - Mean Square Error (MSE) — because precision is key.
  - R² (coefficient of determination) — for those beautiful regression models.
- **Preprocessing**:
  - **Min-Max Normalization**: Ensuring that all input values are scaled between 0 and 1. Because fairness matters, even for data.

## 🏆 Achievements & Learnings

- **Hands-on Training**: No shortcuts here — manually adjusting weights gave deep insights into how neural networks learn.
- **Optimized Predictions**: Multiple models and trials to achieve the best possible prediction accuracy. Spoiler alert: **lower learning rates with higher momentum** work wonders.
- **Data Storytelling**: Visuals and graphs that tell the tale of how insulation and orientation impact heat in homes.
